{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 6 Exercises\n",
    "\n",
    "_McKinney 6.1_\n",
    "\n",
    "There are multiple ways to solve the problems below.  You can use any one of several approaches.  For example, you can read CSV files using Pandas or the csv module.  Your score won't depend on which modules you choose to use unless explicitly noted below, but your programming style will still matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-description",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 30.1 List of Allergies\n",
    "\n",
    "In the /data directory on the Jupyter server, there is a file called `allergies.json` that contains a list of patient allergies.  It is taken from sample data provided by the EHR vendor, Epic, here: https://open.epic.com/Clinical/Allergy\n",
    "\n",
    "Take some time to look at the structure of the file.  You can open it directly in Jupyter by clicking the _Home_ icon, then the _from_instructor_ folder, and then the _data_ folder.\n",
    "\n",
    "Within the file, you'll see that it is a dictionary with many items in it.  One of those items is called `entry` and that item is a list of things.  You can tell that because the item name is immediately followed by an opening square bracket, signifying the start of a list.  It's line 11 of the file: `  \"entry\": [`\n",
    "\n",
    "Write a function named `allergy_count(json_file)` that takes as one parameter the name of the JSON file and returns an integer number of entries in that file.  Your function should open the file, read the json into a Python object, and return how many items there are in the list of `entry`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "\n",
    "ALLERGIES_FILE=\"/data/allergies.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def allergy_count(file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a file as specified by filename and returns how many items exist in the 'entry'\n",
    "    list\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        allergyinfo = json.load(f)\n",
    "        allergylist = allergyinfo.get(\"entry\")\n",
    "     \n",
    "    return len(allergylist)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allergy_count(ALLERGIES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1-tests",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(allergy_count(ALLERGIES_FILE)) == int\n",
    "assert allergy_count(ALLERGIES_FILE) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import doctest\n",
    "#doctest.run_docstring_examples(allergy_count, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.2 Number of Patients\n",
    "\n",
    "If you dig a little bit deaper into this list of allergies, you'll see that each result has a patient associated with it.  Create a funcation called `patient_count(json_file)` that will count how many unique patients we have in this JSON structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def patient_count(file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a file as specified by filename and returns how many items exist in the 'entry'\n",
    "    list\n",
    "    \"\"\"\n",
    "    patients = []\n",
    "    with open(file) as f:\n",
    "        allergyinfo = json.load(f)\n",
    "        entries = allergyinfo.get(\"entry\")\n",
    "        \n",
    "        for entry in entries:\n",
    "            pid = entry.get(\"resource\").get(\"patient\").get(\"reference\")\n",
    "            if pid not in patients:\n",
    "                patients.append(pid)\n",
    "     \n",
    "    return len(patients)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_count(ALLERGIES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30.3 How Many Allergies per Patient\n",
    "\n",
    "Although each entry is a separate allergy, several of them are for the same patient.  Write a function called `allergy_per_patient(json_file)` that counts up how many allergies each patient has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def allergy_per_patient(json_file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a file as specified by filename and returns how many items exist in the 'entry'\n",
    "    list\n",
    "    \"\"\"\n",
    "    patients = {}\n",
    "    with open(json_file) as f:\n",
    "        allergyinfo = json.load(f)\n",
    "        entries = allergyinfo.get(\"entry\")\n",
    "        \n",
    "        for entry in entries:\n",
    "            pid = entry.get(\"resource\").get(\"patient\").get(\"reference\")\n",
    "            pname = entry.get(\"resource\").get(\"patient\").get(\"display\")\n",
    "            if pid not in patients.keys():\n",
    "                patients[pid] = 1\n",
    "            else:\n",
    "                patients[pid] += 1\n",
    "     \n",
    "    return  patients\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://open-ic.epic.com/FHIR/api/FHIR/DSTU2/Patient/Tbt3KuCY0B5PSrJvCu2j-PlK.aiHsu2xUjUM8bWpetXoB': 3,\n",
       " 'https://open-ic.epic.com/FHIR/api/FHIR/DSTU2/Patient/Tbt3KuCY0B5PSrJvCu2j-PlK.aiHsu2xUjUM8bWpetXoZ': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allergy_per_patient(ALLERGIES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-question",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 30.4 Patient Allergies and Reaction\n",
    "\n",
    "You'll see in the file that each of the items in the `entry` list have several other attributes including a patient name, substance text representation, and a reaction manifestation.  Create a function named `allergy_list(json_file)` that will create an output list that has patient name, allergy, and reaction for each `entry`.  The actual result you should get will be:\n",
    "\n",
    "```python\n",
    "[['Jason Argonaut', 'PENICILLIN G', 'Hives'],\n",
    " ['Paul Boal', 'PENICILLIN G', 'Bruising'],\n",
    " ['Jason Argonaut', 'SHELLFISH-DERIVED PRODUCTS', 'Itching'],\n",
    " ['Jason Argonaut', 'STRAWBERRY', 'Anaphylaxis']]\n",
    "```\n",
    "\n",
    "You'll notice that the reaction and the manifestation of that action are lists.  You only need to capture the first reaction and the first manifestation of the action.  That is, if there is a list of things, just output the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "### BEGIN SOLUTION\n",
    "def allergy_list(json_file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a file as specified by filename and returns how many items exist in the 'entry'\n",
    "    list\n",
    "    \"\"\"\n",
    "    patients = []\n",
    "    with open(json_file) as f:\n",
    "        allergyinfo = json.load(f)\n",
    "        entries = allergyinfo.get(\"entry\")\n",
    "        \n",
    "        for entry in entries:\n",
    "            pid = entry.get(\"resource\").get(\"patient\").get(\"reference\")\n",
    "            pname = entry.get(\"resource\").get(\"patient\").get(\"display\")\n",
    "            substance = entry.get(\"resource\").get(\"substance\").get(\"text\")\n",
    "            reaction = entry.get(\"resource\").get(\"reaction\")[0]['manifestation'][0]['text']\n",
    "            patients.append([pname, substance, reaction])\n",
    "     \n",
    "    return  patients\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2-tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "output=[['Jason Argonaut', 'PENICILLIN G', 'Hives'],\n",
    " ['Paul Boal', 'PENICILLIN G', 'Bruising'],\n",
    " ['Jason Argonaut', 'SHELLFISH-DERIVED PRODUCTS', 'Itching'],\n",
    " ['Jason Argonaut', 'STRAWBERRY', 'Anaphylaxis']]\n",
    "\n",
    "assert allergy_list(ALLERGIES_FILE) == output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3-question",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 30.5 Allergy Reaction\n",
    "\n",
    "Write a function called `allergy_reaction(json_file,patient,substance)` that takes three parameter and returns the reaction that will happen if the patient takes the specified substance.  Solve this, in part, by calling your `allergy_list` function inside your new `allergy_reaction` function.\n",
    "\n",
    "If the substance is not found in the allergy list, the function should return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3-answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "### BEGIN SOLUTION\n",
    "def allergy_reaction(json_file, patient, substance):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a file as specified by filename and returns how many items exist in the 'entry'\n",
    "    list\n",
    "    \"\"\"\n",
    "    nidx = 0\n",
    "    subidx = 1\n",
    "    ridx = 2\n",
    "    reaction = None\n",
    "    patient_list = allergy_list(json_file)\n",
    "    for pl in patient_list:\n",
    "        if (pl[nidx] == patient) and (substance == pl[subidx]):\n",
    "            reaction = pl[ridx]\n",
    "\n",
    "    return reaction\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q3-tests",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert allergy_reaction(ALLERGIES_FILE, 'Jason Argonaut', 'PENICILLIN G') == 'Hives'\n",
    "assert allergy_reaction(ALLERGIES_FILE, 'Jason Argonaut', 'SHELLFISH-DERIVED PRODUCTS') == 'Itching'\n",
    "assert allergy_reaction(ALLERGIES_FILE, 'Jason Argonaut', 'STRAWBERRY') == 'Anaphylaxis'\n",
    "assert allergy_reaction(ALLERGIES_FILE, 'Jason Argonaut', 'PENICILLIN') == None\n",
    "assert allergy_reaction(ALLERGIES_FILE, 'Paul Boal', 'PENICILLIN G') == 'Bruising'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Stretch (Extra) Problems\n",
    "\n",
    "Work on either of the stretch problems below can earn you up to 25 free points toward the midterm assignment.  That is, if you complete one of these extra problems successfully, you can skip 1 of the problems that will appear on the midterm exam coming up next week.\n",
    "\n",
    "The midterm will be distributed 3/4 and due 3/14.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### STRETCH for March 2021 - For those looking for an additional challenge\n",
    "\n",
    "As I've mentioned in class, CMS is now enforcing a rule around price transparency.  Every facility that take Medicare payments is required to publish a \"machine readable\" file with it's pricing infomration for a number of common procedures across all of the payers they work with.  There are two examples of such files in the `/data/` directory: `whiteriver.json` and `saline.xml`.\n",
    "\n",
    "If you want to compare contracted prices across these two hospitals, you'll need to read in the information from both of those files into some kind of data structure, then merge the data together from those two files.  See what you can do.\n",
    "\n",
    "See if you can create an output file that has the following fields:\n",
    "* HOSPITAL\n",
    "* PROCEDURE_CODE\n",
    "* PAYER\n",
    "* AMOUNT\n",
    "\n",
    "If you choose to work on this, you may get stuck at some point and you won't know if you're _doing it right_. Make some assumptions. Document your questions in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Procedure Code |  Description  |  Gross Charges  |  Aetna  |  QualChoice\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions:\n",
    "* Only charges > 0 for plans is picked up from the data\n",
    "* DRG data is ignored\n",
    "* The fields below are ingored sicne they appear they are aggregate calculations/statistics on the set of plans.\n",
    "  Description, ProcedureCode, Modifier, RevenueCode, MSDRG, NDC, InpatientGrossCharge, OutpatientGrossCharge, EmergencyRoomGrossCharge, MSDRGAverageGrossCharge, DiscountedCashPrice, MinimumNegotiatedCharge, MaximumNegotiatedCharge\n",
    "### What is NOT done:\n",
    "* Standardization of names of health plans. There appears to be different terminology for the names of plans in the 2 data files. No attempt is made to \n",
    "  standardize it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_price_json(file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a json file and returns a dictionary object with 1 key named \"results\" and the value \n",
    "    being an array of dictionaries\n",
    "\n",
    "    \"\"\"\n",
    "    ignore_keys = [\"Description\", \"ProcedureCode\", \"Modifier\", \"RevenueCode\", \"MSDRG\", \"NDC\", \"InpatientGrossCharge\", \"OutpatientGrossCharge\",\n",
    "                   \"EmergencyRoomGrossCharge\", \"MSDRGAverageGrossCharge\", \"DiscountedCashPrice\", \"MinimumNegotiatedCharge\", \"MaximumNegotiatedCharge\"]\n",
    "    retlist = []\n",
    "    #retlist = {}\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        hospital = data.get(\"root\").get(\"HospitalorFacilityName\")\n",
    "        chargelist = data.get(\"root\").get(\"StandardCharges\")\n",
    "        \n",
    "        for cl in chargelist:\n",
    "            code = cl.get(\"ProcedureCode\")\n",
    "            descr = cl.get(\"Description\")\n",
    "            grosscharge = cl.get(\"OutpatientGrossCharge\")\n",
    "            row = {}\n",
    "            row[\"hospital\"] = hospital\n",
    "            row[\"descr\"] = descr\n",
    "            row[\"code\"] = code\n",
    "            row[\"grosscharge\"] = grosscharge\n",
    "            for plan in cl.keys():\n",
    "                #print(\"Plan is {}\".format(plan))\n",
    "                if plan in ignore_keys:\n",
    "                    continue\n",
    "                #print(cl.get(plan), float(cl.get(plan)))\n",
    "                if float(cl.get(plan)) > 0:\n",
    "                    row[plan] = float(cl.get(plan))\n",
    "                    #print(\"Plan is {}, Price is {}\".format(plan, cl.get(plan)))\n",
    "            #print(row)        \n",
    "            retlist.append(row)\n",
    "            #retlist[row[\"code\"]] = row\n",
    "\n",
    "    return {\"results\": retlist}\n",
    "\n",
    "#print(chargesummary1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT USED \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "import string\n",
    "def summary_price_xml(file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a xml file and returns a dictionary object with 1 key named \"results\" and the value \n",
    "    being an array of dictionaries.\n",
    "    \"\"\"\n",
    "    retlist = []\n",
    "    #retlist = {}\n",
    "    with open(file) as f:\n",
    "        xml = objectify.parse(f)\n",
    "        root = xml.getroot()\n",
    "        row = {}\n",
    "        facroot = root.find(\"Facility\")\n",
    "        #print(facroot.attrib)\n",
    "        #for child in root.getchildren():\n",
    "        #  print(child.tag + \" : \" + str(child.attrib))\n",
    "        #  if child.tag == \"Facility\":\n",
    "        root = facroot\n",
    "        hospital = root.attrib.get(\"Name\")\n",
    "        #print(\"Hospital is {}\".format(hospital))\n",
    "        # Patient Types        \n",
    "        patienttypes = root.findall(\"Patient\")\n",
    "        for pt in patienttypes:\n",
    "            #print(pt.attrib)\n",
    "            patientservicetype = pt.attrib.get(\"Type\")\n",
    "            items = pt.find(\"Charge\").find(\"Item\")\n",
    "            #print(items)\n",
    "            #This is the core loop to get the charges for each item code etc\n",
    "            for i in items:\n",
    "                row = {}\n",
    "                #print(i.attrib)\n",
    "                row[\"hospital\"] = hospital\n",
    "                row[\"descr\"] = i.find(\"Description\")\n",
    "                row[\"code\"] = i.attrib.get(\"Code\")\n",
    "                row[\"grosscharge\"] = i.find(\"GrossCharge\")\n",
    "                print(type(row[\"hospital\"]))\n",
    "                print(type(row[\"grosscharge\"]))\n",
    "                for c in i.find(\"Contracts\").findall(\"Contract\"):\n",
    "                    #print(\"Contract is {}\".format(c.attrib.get(\"Payer\")))\n",
    "                    row[c.attrib.get(\"Payer\") + \"_\" + patientservicetype] = c.attrib.get(\"Charge\") \n",
    "                #print(row)\n",
    "                #print(json.dumps(row, indent=4))\n",
    "                retlist.append(row)\n",
    "                #retlist[row[\"code\"]] = row\n",
    "                break\n",
    "    return {\"results\": retlist}\n",
    "        \n",
    "#print(chargesummary2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions:\n",
    "* A string is constructued thus \"healthplanname_patientservicetype\" for an entry against the code\n",
    "* Only Charge types of HCPPS are considered. DRG types are not considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "def get_contracts(contract, patientservicetype):\n",
    "    \"\"\"(contract) -> dict json object which could be a dict or an array\n",
    "    (patientservicetype) -> string Patient Service type ie. emergency, inpatient etc.\n",
    "    Rturns a dictionary of K-V pairs with the payer as the key and the charge as the value.\n",
    "    \"\"\"\n",
    "    #print(\"get contracts {}\".format(contract))\n",
    "    val = {}\n",
    "    #print(type(contract))\n",
    "    if isinstance(contract, dict):\n",
    "        #print(\"There is only 1 contract\")\n",
    "        val[contract[\"@Payer\"]+\"_\"+patientservicetype] = contract[\"@Charge\"]\n",
    "    elif type(contract) is list:\n",
    "        #print(\"There is a list of contracts\")\n",
    "        for co in contract:\n",
    "            val[co[\"@Payer\"]+\"_\"+patientservicetype] = co[\"@Charge\"]\n",
    "    #print(val)\n",
    "    return val\n",
    "\n",
    "def summary_price_xmljson(file):\n",
    "    \"\"\"(file) -> filename\n",
    "    This function opens a xml file and returns a dictionary object with 1 key named \"results\" and the value \n",
    "    being an array of dictionaries. The approach taken is to convert an xml object to json.\n",
    "    \"\"\"\n",
    "    with open(\"/data/saline.xml\") as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "        #print(json.dumps(doc, indent = 4))\n",
    "        retlist = []\n",
    "        patientarray = doc.get(\"StandardCharges\").get(\"Facility\").get(\"Patient\")\n",
    "        hospital = doc.get(\"StandardCharges\").get(\"Facility\").get(\"@Name\")\n",
    "        for p in patientarray:\n",
    "            patientservicetype = p.get(\"@Type\")\n",
    "            charges = p.get(\"Charge\")\n",
    "            #print(\"Type for charges is {} \\n\".format(type(charges)))\n",
    "            if type(charges) == list:\n",
    "                for c in charges:\n",
    "                    print(c.get(\"@Type\"))\n",
    "                    #print(\"C is {}\".format(c))\n",
    "                    if c.get(\"@Type\") == \"DRG\":\n",
    "                        #print(\"ON TO THE NEXT BECAUSE THIS IS FOR DRG\")\n",
    "                        continue\n",
    "                    #Only do HCPCS types\n",
    "                    for i in c.get(\"Item\"):\n",
    "                        row = {}\n",
    "                        row[\"hospital\"] = hospital\n",
    "                        row[\"descr\"] = i.get(\"Description\")\n",
    "                        row[\"code\"] = i.get(\"@Code\")\n",
    "                        row[\"grosscharge\"] = i.get(\"GrossCharge\")\n",
    "                        \n",
    "                        contract = i.get(\"Contracts\").get(\"Contract\")\n",
    "                        #print(\"Contract 1 is {}\".format(contract))\n",
    "                        row2 = get_contracts(contract, patientservicetype)\n",
    "                        #print(\"Row is {} and row2 is {}\".format(row, row2))\n",
    "                        row = {**row,**row2}\n",
    "                        #print(\"Row is {}\".format(row))\n",
    "                        retlist.append(row)\n",
    "            else:\n",
    "                if c.get(\"@Type\") == \"DRG\":\n",
    "                    continue\n",
    "                #Only do HCPCS types\n",
    "                for i in c.get(\"Item\"):\n",
    "                    row = {}\n",
    "                    row[\"hospital\"] = hospital\n",
    "                    row[\"descr\"] = i.get(\"Description\")\n",
    "                    row[\"code\"] = i.get(\"@Code\")\n",
    "                    row[\"grosscharge\"] = i.get(\"GrossCharge\")\n",
    "                    contract = i.get(\"Contracts\").get(\"Contract\")\n",
    "                    #print(\"Contract 2 is {}\".format(contract))\n",
    "                    row2 = get_contracts(contract, patientservicetype)\n",
    "\n",
    "                    row = {**row,**row2}\n",
    "                    #print(\"Row is {}\".format(row))\n",
    "                    retlist.append(row)\n",
    "                        \n",
    "        return {\"results\": retlist}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results\n",
    "The output expected in this section are 1 csv file named \"summary.csv\"\n",
    "This has a summary of all the charges\n",
    "- No DRG Data\n",
    "- No standardization of plan names\n",
    "- No sorting/arrangement of data in the output.\n",
    "It is expected that an Excel/google sheet user can use the appropriate filters to study the csv and compare plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRG\n",
      "HCPCS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chargesummary1 = summary_price_json(\"/data/whiteriver.json\") \n",
    "df = pd.json_normalize(chargesummary1['results'])\n",
    "df.to_csv(\"samplecsv1.csv\")\n",
    "\n",
    "#print(json.dumps(chargesummary1, indent = 4))\n",
    "chargesummary2 = summary_price_xmljson(\"/data/saline.xml\") \n",
    "df = pd.json_normalize(chargesummary2['results'])\n",
    "df.to_csv(\"samplecsv2.csv\")\n",
    "#print(chargesummary2)\n",
    "#print(json.dumps(chargesummary2, indent = 4))\n",
    "\n",
    "#Merge the 2 returned structures \n",
    "mergeddata = chargesummary1\n",
    "mergeddata[\"results\"] = mergeddata[\"results\"] + chargesummary2[\"results\"]\n",
    "df = pd.json_normalize(mergeddata['results'])\n",
    "df.to_csv(\"summary.csv\")\n",
    "\n",
    "#print(json.dumps(mergeddata, indent = 4))\n",
    "\n",
    "#mergeddata[\"results\"].append(chargesummary2[\"results\"])\n",
    "#print(json.dumps(mergeddata, indent = 4))\n",
    "\n",
    "#import pandas as pd\n",
    "#import json\n",
    "\n",
    "#info = json.loads(chargesummary1)\n",
    "#df = pd.json_normalize(mergeddata['results'])\n",
    "#df.to_csv(\"samplecsv1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "* What is MSDRG and DRG in the 2 data files\n",
    "* How to standardize plan names for ease of comparison ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### STRETCH from March 2020 - For those looking for an additional challenge\n",
    "\n",
    "The Coronavirus is creating quite the stir right now.  There are some sources suggesting that trends show it is going to be significantly more serious than SARS was back in the 2002 timeframe.  Here's one visualization trying to demonstrate that: https://www.reddit.com/r/China_Flu/comments/ev2b4v/i_updated_some_charts_comparing_this_outbreak/\n",
    "\n",
    "Someone on Kaggle has generously already compiled a dataset based on information from Johns Hopkins about the Coronavirus outbreak.  https://www.kaggle.com/brendaso/2019-coronavirus-dataset-01212020-01262020  Create a Kaggle account, if you don't already have one.  Download this data set and then upload it to your Jupyter Home folder.  (The \"up arrow\" button is for uploading a file.)\n",
    "\n",
    "Use Python's built-in `csv` module to read the data from this file and generate the following information: **what are the total confirmed cases in all of Mainland China as of the latest information in the data set?**  Some important things to note:\n",
    "* Each entry for a given city has the **cumulative** number of cases.  So that column is not additive (it cannot be summed).  You'll have to find a way to filter your data for the last day for each city, then total those up.\n",
    "* If you choose to parse the date column, you will want to lookup how to do that using Python's `datetime` module.  Especially the `strptime` function.  https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior  Hint: you can parse a date string in the format 2/17/2020 using the code below.  This link will tell you what things like `%m` and `%Y` mean.\n",
    "\n",
    "```\n",
    "from datetime import datetime\n",
    "d = datetime.strptime('2/17/2020', '%m/%d/%Y')\n",
    "```\n",
    "\n",
    "If you want to take this another step, **create a list of tuples that contain (observate date, total confirmed) totalled over all locations represented in the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Check your work above\n",
    "\n",
    "If you didn't get them all correct, take a few minutes to think through those that aren't correct.\n",
    "\n",
    "\n",
    "## Submitting Your Work\n",
    "\n",
    "In order to submit your work, you'll need to use the `git` command line program to **add** your homework file (this file) to your local repository, **commit** your changes to your local repository, and then **push** those changes up to github.com.  From there, I'll be able to **pull** the changes down and do my grading.  I'll provide some feedback, **commit** and **push** my comments back to you.  Next week, I'll show you how to **pull** down my comments.\n",
    "\n",
    "To run through everything one last time and submit your work:\n",
    "1. Use the `Kernel` -> `Restart Kernel and Run All Cells` menu option to run everything from top to bottom and stop here.\n",
    "2. Follow the instruction on the prompt below to either ssave and submit your work, or continue working.\n",
    "\n",
    "If anything fails along the way with this submission part of the process, let me know.  I'll help you troubleshoort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "a=input('''\n",
    "Are you ready to submit your work?\n",
    "1. Click the Save icon (or do Ctrl-S / Cmd-S)\n",
    "2. Type \"yes\" or \"no\" below\n",
    "3. Press Enter\n",
    "\n",
    "''')\n",
    "\n",
    "if a=='yes':\n",
    "    !git add week06_assignment_2.ipynb\n",
    "    !git commit -a -m \"Submitting the week 6 programming exercises\"\n",
    "    !git push\n",
    "else:\n",
    "    print('''\n",
    "    \n",
    "OK. We can wait.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "If the message above says something like _Submitting the week 3 review exercises_ or _Everything is up to date_, then your work was submitted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
